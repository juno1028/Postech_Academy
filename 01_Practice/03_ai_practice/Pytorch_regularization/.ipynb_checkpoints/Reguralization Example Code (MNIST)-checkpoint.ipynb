{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reguralization (Dropout, Normalization)\n",
    "> ## 1. Library\n",
    "\n",
    "> ## 2. MLP naive setting & Pre-process data\n",
    "\n",
    "> ## 3. MLP batch norm\n",
    ">> ### 3.1 with test phase\n",
    ">> ### 3.2 with train, test phase\n",
    "\n",
    "> ## 4. Dropout\n",
    "\n",
    "> ## 5. Batch norm & Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: gpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"device: gpu\") if torch.cuda.is_available() else print(\"device: cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyper parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# hypter parameter setting\n",
    "learning_rate = 1e-1\n",
    "epochs = 30\n",
    "# batch_size = 60000 # gradient descent\n",
    "# batch_size = 1 # stochastic gradient descent\n",
    "batch_size = 32 # mini-batch stochastic gradient descent\n",
    "act = nn.Tanh()\n",
    "h = 200\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Load data & Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data):  60000\n",
      "len(test_data):  10000\n",
      "original data shape:  torch.Size([1, 28, 28])\n",
      "label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed data shape: torch.Size([32, 1, 28, 28])\n",
      "label: tensor([9, 6, 6, 8, 5, 7, 1, 5, 5, 4, 7, 0, 6, 3, 3, 3, 7, 4, 7, 1, 7, 9, 1, 4,\n",
      "        7, 9, 6, 0, 2, 0, 6, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "train_data = dataset.MNIST(\"./\", train = True, transform = transforms.ToTensor(), target_transform = None, download = True)\n",
    "test_data = dataset.MNIST(\"./\", train = False, transform = transforms.ToTensor(), target_transform = None, download = True)\n",
    "\n",
    "# check the data\n",
    "print('len(train_data): ', len(train_data))\n",
    "print('len(test_data): ', len(test_data))\n",
    "\n",
    "x_train, y_train = train_data[0]\n",
    "print('original data shape: ', x_train.shape)\n",
    "print('label: ', y_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(x_train[0])\n",
    "plt.show()\n",
    "\n",
    "# Pre-process (batch, shuffle)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True, num_workers = 1, drop_last = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 10000, shuffle = True, num_workers = 1, drop_last = True)\n",
    "\n",
    "# check the data \n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_target) = next(examples)\n",
    "\n",
    "print('processed data shape:', example_data.shape)\n",
    "print('label:', example_target)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(example_data[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Model & Optimization and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = nn.Sequential(\n",
    "        nn.Linear(np.prod(x_train.shape[1:]),1024),\n",
    "        act,\n",
    "        nn.Linear(1024,300),\n",
    "        act,\n",
    "        nn.Linear(300,10)\n",
    "        )\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "# loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \\Loss: tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_array = []\n",
    "iteration_loss_array = []\n",
    "\n",
    "# train the model\n",
    "for epoch in range(epochs):\n",
    "    for iteration, [data, label] in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = data.to(device)\n",
    "        x = x.view(batch_size, -1)\n",
    "        y = label.to(device)\n",
    "        \n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iteration_loss_array.append(loss.cpu().detach().numpy())\n",
    "            \n",
    "    loss_array.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch:\", epoch + 1, \"\\Loss:\", loss)\n",
    "\n",
    "# test\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "prediction_list = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        x = data.to(device)\n",
    "        x = x.view(-1, 784)\n",
    "        y = label.to(device)\n",
    "        \n",
    "        prediction = model(x)\n",
    "        _, prediction_index = torch.max(prediction, 1)\n",
    "                \n",
    "        prediction_list.append(prediction_index)\n",
    "        label_list.append(y)\n",
    "        \n",
    "        total += y.size(0)\n",
    "        correct += (prediction_index == y).sum().float()\n",
    "        \n",
    "print('total', total)\n",
    "print('correct', correct)\n",
    "print('accuracy', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Plot result & Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.figure()\n",
    "plt.plot(loss_array)\n",
    "plt.show()\n",
    "# plot iteration losses\n",
    "plt.figure()\n",
    "plt.plot(iteration_loss_array)\n",
    "plt.show()\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "    \n",
    "prediction_array = np.array(prediction_list[0].cpu())\n",
    "label_array = np.array(label_list[0].cpu())\n",
    "\n",
    "print(\"prediction :\", prediction_array.shape)\n",
    "print(\"true label :\", label_array.shape)\n",
    "\n",
    "confusion_matrix(\n",
    "    label_array,\n",
    "    prediction_array) # y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build FC Layer Model\n",
    "## (Show inner distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = 1024\n",
    "        self.hidden2 = 300\n",
    "        self.final = 10\n",
    "\n",
    "        self.fc1 = nn.Linear(784, self.hidden1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(self.hidden1, self.hidden2)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(self.hidden2, self.final)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = self.fc1(x)\n",
    "        hidden1 = self.act1(output1)\n",
    "\n",
    "        output2 = self.fc2(hidden1)\n",
    "        hidden2 = self.act2(output2)\n",
    "        \n",
    "        logit = self.fc3(hidden2)\n",
    "        \n",
    "        return output1, hidden1, output2, hidden2, logit\n",
    "\n",
    "model = MLP().to(device)\n",
    "model.train()\n",
    "\n",
    "# loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loss_array = []\n",
    "\n",
    "# train the model\n",
    "for i in range(epochs):\n",
    "    for index, [data, label] in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(batch_size, -1)\n",
    "        output1, hidden1, output2, hidden2, logit = model.forward(data)\n",
    "        loss = loss_function(logit, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_array.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    if i % display_step == 0:\n",
    "        print('{} epoch loss: {}'.format(i,loss))\n",
    "        \n",
    "# test\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "prediction_list = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        x = data.to(device)\n",
    "        x = x.view(-1, 784)\n",
    "        y = label.to(device)\n",
    "        \n",
    "        output1, hidden1, output2, hidden2, prediction = model(x)\n",
    "        _, prediction_index = torch.max(prediction, 1)\n",
    "                \n",
    "        prediction_list.append(prediction_index)\n",
    "        label_list.append(y)\n",
    "        \n",
    "        total += y.size(0)\n",
    "        correct += (prediction_index == y).sum().float()\n",
    "        \n",
    "print('total', total)\n",
    "print('correct', correct)\n",
    "print('accuracy', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_array)\n",
    "plt.show()\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "    \n",
    "prediction_array = np.array(prediction_list[0].cpu())\n",
    "label_array = np.array(label_list[0].cpu())\n",
    "\n",
    "print(\"prediction :\", prediction_array.shape)\n",
    "print(\"true label :\", label_array.shape)\n",
    "\n",
    "confusion_matrix(\n",
    "    label_array,\n",
    "    prediction_array) # y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Show inner distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(x.shape, output1.shape, hidden1.shape, output2.shape, hidden2.shape)\n",
    "\n",
    "def plot_inner_dist(x, o1, h1, o2, h2):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 7), sharex='col')\n",
    "    axs[0, 0].hist(o1.reshape(-1))\n",
    "    axs[0, 1].hist(h1.reshape(-1))\n",
    "    axs[0, 2].scatter(o1[0], h1[0])\n",
    "    axs[1, 0].hist(o2.reshape(-1))\n",
    "    axs[1, 1].hist(h2.reshape(-1))\n",
    "    axs[1, 2].scatter(o2[0], h2[0])\n",
    "    plt.show()\n",
    "plot_inner_dist(x.cpu().numpy(), output1.cpu().numpy(), \n",
    "                hidden1.cpu().numpy(), output2.cpu().numpy(), hidden2.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Add Batch Normalize Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = 1024\n",
    "        self.hidden2 = 300\n",
    "        self.final = 10\n",
    "\n",
    "        self.fc1 = nn.Linear(784, self.hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(self.hidden1, self.hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.hidden2)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(self.hidden2, self.final)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = self.fc1(x)\n",
    "        bn1 = self.bn1(output1)\n",
    "        hidden1 = self.act1(bn1)\n",
    "\n",
    "        output2 = self.fc2(hidden1)\n",
    "        bn2 = self.bn2(output2)\n",
    "        hidden2 = self.act2(bn2)\n",
    "        \n",
    "        logit = self.fc3(hidden2)\n",
    "        \n",
    "        return output1, hidden1, output2, hidden2, logit\n",
    "\n",
    "model = MLP().to(device)\n",
    "model.train()\n",
    "\n",
    "# loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loss_array = []\n",
    "\n",
    "# train the model\n",
    "for i in range(epochs):\n",
    "    for index, [data, label] in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(batch_size, -1)\n",
    "        output1, hidden1, output2, hidden2, logit = model.forward(data)\n",
    "        loss = loss_function(logit, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_array.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    if i % display_step == 0:\n",
    "        print('{} epoch loss: {}'.format(i,loss))\n",
    "        \n",
    "        \n",
    "# test\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "prediction_list = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        x = data.to(device)\n",
    "        x = x.view(-1, 784)\n",
    "        y = label.to(device)\n",
    "        \n",
    "        output1, hidden1, output2, hidden2, prediction = model(x)\n",
    "        _, prediction_index = torch.max(prediction, 1)\n",
    "                \n",
    "        prediction_list.append(prediction_index)\n",
    "        label_list.append(y)\n",
    "        \n",
    "        total += y.size(0)\n",
    "        correct += (prediction_index == y).sum().float()\n",
    "        \n",
    "print('total', total)\n",
    "print('correct', correct)\n",
    "print('accuracy', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_array)\n",
    "plt.show()\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "    \n",
    "prediction_array = np.array(prediction_list[0].cpu())\n",
    "label_array = np.array(label_list[0].cpu())\n",
    "\n",
    "print(\"prediction :\", prediction_array.shape)\n",
    "print(\"true label :\", label_array.shape)\n",
    "\n",
    "confusion_matrix(\n",
    "    label_array,\n",
    "    prediction_array) # y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Show inner distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, output1.shape, hidden1.shape, output2.shape, hidden2.shape)\n",
    "\n",
    "def plot_inner_dist(x, o1, h1, o2, h2):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 7), sharex='col')\n",
    "    axs[0, 0].hist(o1.reshape(-1))\n",
    "    axs[0, 1].hist(h1.reshape(-1))\n",
    "    axs[0, 2].scatter(o1[0], h1[0])\n",
    "    axs[1, 0].hist(o2.reshape(-1))\n",
    "    axs[1, 1].hist(h2.reshape(-1))\n",
    "    axs[1, 2].scatter(o2[0], h2[0])\n",
    "    plt.show()\n",
    "    \n",
    "plot_inner_dist(x.cpu().numpy(), output1.cpu().numpy(), \n",
    "                hidden1.cpu().numpy(), output2.cpu().numpy(), hidden2.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Add Dropout Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# hyper parameter setting\n",
    "learning_rate = 1e-1\n",
    "epochs = 100\n",
    "# batch_size = 60000 # gradient descent\n",
    "# batch_size = 1 # stochastic gradient descent\n",
    "batch_size = 32 # mini-batch stochastic gradient descent\n",
    "act = nn.Tanh()\n",
    "h = 200\n",
    "display_step = 10\n",
    "dropout_rate = .2 # probability to be 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = 1024\n",
    "        self.hidden2 = 300\n",
    "        self.final = 10\n",
    "\n",
    "        self.fc1 = nn.Linear(784, self.hidden1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.drop1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(self.hidden1, self.hidden2)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.drop2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(self.hidden2, self.final)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = self.fc1(x)\n",
    "        hidden1 = self.act1(output1)\n",
    "        drop1 = self.drop1(hidden1)\n",
    "        \n",
    "        output2 = self.fc2(drop1)\n",
    "        hidden2 = self.act2(output2)\n",
    "        drop2 = self.drop2(hidden2)\n",
    "        \n",
    "        logit = self.fc3(drop2)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "model = MLP().to(device)\n",
    "model.train()\n",
    "\n",
    "# loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loss_array = []\n",
    "\n",
    "# train the model\n",
    "for i in range(epochs):\n",
    "    for index, [data, label] in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(batch_size, -1)\n",
    "        logit = model.forward(data)\n",
    "        loss = loss_function(logit, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_array.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    if i % display_step == 0:\n",
    "        print('{} epoch loss: {}'.format(i,loss))\n",
    "        \n",
    "        \n",
    "# test\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "prediction_list = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        x = data.to(device)\n",
    "        x = x.view(-1, 784)\n",
    "        y = label.to(device)\n",
    "        \n",
    "        prediction = model(x)\n",
    "        _, prediction_index = torch.max(prediction, 1)\n",
    "                \n",
    "        prediction_list.append(prediction_index)\n",
    "        label_list.append(y)\n",
    "        \n",
    "        total += y.size(0)\n",
    "        correct += (prediction_index == y).sum().float()\n",
    "        \n",
    "print('total', total)\n",
    "print('correct', correct)\n",
    "print('accuracy', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_array)\n",
    "plt.show()\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "    \n",
    "prediction_array = np.array(prediction_list[0].cpu())\n",
    "label_array = np.array(label_list[0].cpu())\n",
    "\n",
    "print(\"prediction :\", prediction_array.shape)\n",
    "print(\"true label :\", label_array.shape)\n",
    "\n",
    "confusion_matrix(\n",
    "    label_array,\n",
    "    prediction_array) # y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Add Batch norm, Dropout Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = 1024\n",
    "        self.hidden2 = 300\n",
    "        self.final = 10\n",
    "\n",
    "        self.fc1 = nn.Linear(784, self.hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.drop1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.hidden1, self.hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.hidden2)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.drop2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.hidden2, self.final)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = self.fc1(x)\n",
    "        bn1 = self.bn1(output1)\n",
    "        hidden1 = self.act1(bn1)\n",
    "        drop1 = self.drop1(hidden1)\n",
    "\n",
    "        output2 = self.fc2(drop1)\n",
    "        bn2 = self.bn2(output2)\n",
    "        hidden2 = self.act2(bn2)\n",
    "        drop2 = self.drop2(hidden2)\n",
    "        \n",
    "        logit = self.fc3(drop2)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "model = MLP().to(device)\n",
    "model.train()\n",
    "\n",
    "# loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loss_array = []\n",
    "\n",
    "# train the model\n",
    "for i in range(epochs):\n",
    "    for index, [data, label] in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(batch_size, -1)\n",
    "        logit = model.forward(data)\n",
    "        loss = loss_function(logit, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_array.append(loss.cpu().detach().numpy())\n",
    "    \n",
    "    if i % display_step == 0:\n",
    "        print('{} epoch loss: {}'.format(i,loss))\n",
    "        \n",
    "        \n",
    "# test\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "prediction_list = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        x = data.to(device)\n",
    "        x = x.view(-1, 784)\n",
    "        y = label.to(device)\n",
    "        \n",
    "        prediction = model(x)\n",
    "        _, prediction_index = torch.max(prediction, 1)\n",
    "                \n",
    "        prediction_list.append(prediction_index)\n",
    "        label_list.append(y)\n",
    "        \n",
    "        total += y.size(0)\n",
    "        correct += (prediction_index == y).sum().float()\n",
    "        \n",
    "print('total', total)\n",
    "print('correct', correct)\n",
    "print('accuracy', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_array)\n",
    "plt.show()\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "    \n",
    "prediction_array = np.array(prediction_list[0].cpu())\n",
    "label_array = np.array(label_list[0].cpu())\n",
    "\n",
    "print(\"prediction :\", prediction_array.shape)\n",
    "print(\"true label :\", label_array.shape)\n",
    "\n",
    "confusion_matrix(\n",
    "    label_array,\n",
    "    prediction_array) # y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Advanced: batch norm + dropout in CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True, num_workers = 1, drop_last = True)\n",
    "\n",
    "# batchnorm + dropout in CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding =1), # 28 x 28\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(16, 32, 3, padding =1), # 14 x 14 \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2) # 7 x 7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32*7*7,100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feature_extraction = self.feature_extraction(x)\n",
    "        flatten = feature_extraction.view(batch_size, -1)         \n",
    "        logit = self.classifier(flatten)\n",
    "        \n",
    "        return logit\n",
    "        \n",
    "model = CNN().to(device)\n",
    "model.train()\n",
    "\n",
    "# loss and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_array = []\n",
    "\n",
    "# train the model\n",
    "for i in range(epochs):\n",
    "    for index, [data, label] in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(data)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i % display_step == 0:\n",
    "        print('{} epoch loss: {}'.format(i,loss))\n",
    "        loss_array.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "#test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "prediction_list = []\n",
    "label_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, [data, label] in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        output = model.forward(data)\n",
    "        _, prediction_index = torch.max(output, 1)\n",
    "        \n",
    "        prediction_list.append(prediction_index)\n",
    "        label_list.append(label)\n",
    "        \n",
    "        total += label.size(0)\n",
    "        correct += (prediction_index == label).sum().float()\n",
    "\n",
    "    print(\"Accuracy of the model: {}\".format(correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
